# Run Flower in Docker container on multiple Devices

## 1. Preparing the Swarm Cluster

### On the Server (Manager Node):

1. **Initialize Swarm:**
    
    ```bash
    docker swarm init
    ```
    
    - The server becomes the **Manager Node**.
    - The command returns a **Join Token**, which is used to add clients to the cluster.
2. **Display Join Token (if lost):**
    
    ```bash
    docker swarm join-token worker
    ```
    
    - This command shows the token that clients can use to join the Swarm cluster.

---

### On the Clients (Worker Nodes):

1. **Join Swarm:**
    
    Run the following command on each client:
    
    ```bash
    docker swarm join --token <TOKEN> <MANAGER-IP>:2377
    ```
    
    - **`<TOKEN>`**: The token generated by the manager.
    - **`<MANAGER-IP>`**: The IP address of the Manager Node.
2. **Verify Connection:**
    
    On the server, check if the clients have successfully joined:
    
    ```bash
    docker node ls
    ```
    
    - All nodes (manager and workers) should be listed.

---

## 2. Changing Hostnames on Raspberry Pis

### On Each Raspberry Pi:

1. **Check Current Hostname:**
    
    ```bash
    hostname
    ```
    
2. **Change Hostname:**
    
    Run the following command on each device, replacing `<new-hostname>` with the desired name:
    
    ```bash
    sudo hostnamectl set-hostname <new-hostname>
    ```
    
    Examples:
    
    - **On the Manager:**
        
        ```bash
        sudo hostnamectl set-hostname manager-node
        ```
        
    - **On the First Worker:**
        
        ```bash
        sudo hostnamectl set-hostname worker-node-1
        ```
        
3. **Restart the Device:**
    
    ```bash
    sudo reboot
    ```
    

---

## **3. Preparing Docker Images**

### Option 1: Build Images Locally on Each Server and Client

1. **Prepare Dockerfile:**
    
    Ensure that the `Dockerfile` for `clientapp` and `serverapp` are available on the respective devices.
    
2. **Build Images:**
    
    For the Client App:
    
    ```bash
    docker build -t flwr_clientapp -f clientapp.Dockerfile .
    ```
    
    For the Server App:
    
    ```bash
    docker build -t flwr_serverapp -f serverapp.Dockerfile .
    ```
    
3. **Verify Availability:**
    
    ```bash
    docker images
    ```
    

---

### Option 2: Build images on the host/server and distribute them via a registry

Alternatively, the Docker images can be built centrally on the manager node or a dedicated machine and uploaded to a container registry (e.g. Harbor). The worker nodes can then obtain the images from there.

1. **Build images:**
    
    For the client app:
    
    ```bash
    docker build -t harbor.com/${project}/flwr_clientapp -f clientapp.Dockerfile .
    ```
    
    For the server app:
    
    ```bash
    docker build -t harbor.com/${project}/flwr_serverapp -f serverapp.Dockerfile .
    ```
    
2.  **Log in to the registry:**
It is ensured that a registration with the registry (e.g. Harbor) has taken place. If necessary, the following command is executed:
    
    ```bash
    docker login harbor.com
    ```
    
3. **Push images to the registry:**
After the build process, the images are uploaded to the registry:
    
    For the client app:
    
    ```bash
    docker push harbor.com/${project}/flwr_clientapp
    ```
    
    For the server app:
    
    ```bash
    docker push harbor.com/${project}/flwr_serverapp
    ```
    
4. **Pull images from the registry:**
If the Docker images are in the registry, they can be downloaded to the corresponding devices:
    
    For the client app:
    
    ```bash
    docker pull harbor.com/${project}/flwr_clientapp
    ```
    
    For the server app:
    
    ```bash
    docker pull harbor.com/${project}/flwr_serverapp
    ```
    
    - **Attention:** Different architectures of systems on which the image is built and on which the container is built, e.g. x86_64 (`amd64`) architecture or ARM (`arm64` or `armhf`). Possible with Crosscompiler Docker Buildx for multi-arch support:
        
        ### **Enable Buildx** (if not already enabled)
        
        ```bash
        docker buildx create --use
        docker buildx inspect --bootstrap
        ```
        
        ### **Build image and push to the registry**
        
        **For arm64 or armhf:**
        
        ```bash
        docker buildx build --platform linux/arm64 -t harbor.com/${project}/flwr_serverapp:latest -f serverapp.Dockerfile --push .
        ```
        
        **For x86_64 (amd64):**
        
        ```bash
        docker buildx build --platform linux/amd64 -t harbor.com/${project}/flwr_serverapp:latest -f serverapp.Dockerfile --push .
        ```
        
        ðŸ’¡ Note: The image must be pushed to a registry, as Buildx does not save it locally.

---

## 4. Creating an Overlay Network

Create an overlay network that is shared between all nodes, this is defined in the compose.yaml:

```bash
docker network create \
  --driver overlay \
  my_overlay_network
```

This network enables communication between containers regardless of which node they are running on.

---

## 5. Defining Docker Compose Stack

Example `docker-compose.yml`:

```yaml
services:
  superlink:
    image: flwr/superlink:1.14.0
    command:
      - --insecure
      - --isolation
      - process
    ports:
      - 9091:9091
      - 9092:9092
      - 9093:9093
    networks:
      - flwr-overlay-network
    # Optional: Define which device should run this container
    deploy:
      placement:
        constraints:
          - node.hostname == manager-node  # Execute on the manager node

  supernode-1:
    image: flwr/supernode:1.14.0
    command:
      - --insecure
      - --superlink
      - superlink:9092
      - --node-config
      - "partition-id=0 num-partitions=2"
      - --clientappio-api-address
      - 0.0.0.0:9094
      - --isolation
      - process
    ports:
      - 9094:9094
    networks:
      - flwr-overlay-network
    depends_on:
      - superlink
    # Optional: Define which device should run this container
    deploy:
      placement:
        constraints:
          - node.hostname == worker-node-1  # Execute on worker-node-1

  serverapp:
    image: harbor.com/${project}/flwr_serverapp:0.0.1
    command:                         
      - --insecure
      - --serverappio-api-address
      - superlink:9091
    networks:
      - flwr-overlay-network
    depends_on:
      - superlink
    # Optional: Define which device should run this container
    deploy:
      placement:
        constraints:
          - node.hostname == manager-node  # Execute on the manager node

  clientapp-1:
    image: harbor.com/${project}/flwr_clientapp:0.0.1
    command:                         
      - --insecure
      - --clientappio-api-address
      - supernode-1:9094
    networks:
      - flwr-overlay-network
    depends_on:
      - supernode-1
    # Optional: Define which device should run this container
    deploy:
      placement:
        constraints:
          - node.hostname == worker-node-1  # Execute on worker-node-1

networks:
  flwr-overlay-network:
    driver: overlay   Use an overlay network for communication between services across multiple nodes
```

### **Explanation:**

- **`deploy.replicas`**: Specifies the number of instances for each service.
- **`placement.constraints`**: Controls where the containers are executed:
    - `node.role == worker`: Only on worker nodes.
    - `node.role == manager`: Only on the manager.
- **`networks`**: Connects the containers via the overlay network.

---

## 6. Installing Local Libraries

Install dependencies locally:

```bash
pip install -e .
```

---

## 7. Deploying the Stack

On the Manager Node:

```bash
docker stack deploy -c compose.yml my_stack
```

- **`my_stack`**: The name of the stack under which the services are managed.
- Swarm automatically distributes the containers to the nodes based on the `docker-compose.yml`.

---

## 8. Checking Status

### List Services:

On the manager node:

```bash
docker service ls
```

- Shows all services in the stack and their status (e.g. number of replicas).

### Check Nodes:

```bash
docker node ls
```

- Shows the nodes in the cluster and their status (e.g. ACTIVE).

### List Running Containers:

1. On the manager:
    
    ```bash
    docker ps
    ```
    
    Displays the containers running on the Manager.
    
2. On the worker nodes:
    - Log in to the clients and run `docker ps` to see which containers are running there.

---

## 9. Running Flower

Starting the flower run on the server or host device:

```bash
flwr . run local-deployment --stream
```

---

## 10. Viewing Logs

```bash
docker service logs flwr_clientapp
```
